{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGALqNFcIC5ZyIcimoFJZ0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PARTHIBAN-007/SLM-From-Scratch/blob/main/SLM_From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wIV-xQ7oLNe"
      },
      "outputs": [],
      "source": [
        "!pip install datasets tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"roneneldan/TinyStories\")"
      ],
      "metadata": {
        "id": "XSylDa1foX_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "en = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "def process(example):\n",
        "  ids = enc.encode_ordinary(example['text'])\n",
        "  out = {'ids':ids,'len':len(ids)}\n",
        "  return out\n",
        "\n",
        "\n",
        "if not os.path.exists('train.bin'):\n",
        "  tokenized = ds.map(\n",
        "      process,\n",
        "      remove_columns = ['text'],\n",
        "      desc = \"tokenizing the splits\",\n",
        "      num_proc = 8\n",
        "  )\n",
        "\n",
        "  for split,dset in tokenized.items():\n",
        "    arr_len = np.sum(dset['len'],dtype = np.uint64)\n",
        "    filename = f'{split}.bin'\n",
        "    dtype = np.uint16\n",
        "    arr = np.memmap(filename,dtype = dtype,mode = \"w+\",shape = (arr_len,))\n",
        "    total_batches = 1024\n",
        "\n",
        "    idx = 0\n",
        "    for batch_idx in tqd(range(total_batches),desc = f'writing {filename}'):\n",
        "      batch = dset.shard(num_shards = total_batches,index = batch_idx , contiguous = True).with_format('numpy')\n",
        "      arr_batch=  np.concatenate(batch['ids'])\n",
        "      arr[idx:idx+len(arr_batch)] = arr_batch\n",
        "      idx += len(arr_batch)\n",
        "    arr.flush()\n"
      ],
      "metadata": {
        "id": "KSyY8hEJol-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "  if split == \"train\":\n",
        "    data = np.memmap('train.bin',dtype = np.uint16,mode = 'r')\n",
        "  else:\n",
        "    data = np.memmap('validation.bin',dtype=np.uint16,mode = 'r')\n",
        "  ix = torch.randint(len(data)-block_size,(batch_size,))\n",
        "  x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "  y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "\n",
        "  if device_type == \"cuda\":\n",
        "    x,y = x.pin_memory().to(device,non_blocking = True), y.pin_memory().to(device,non_blocking = True)\n",
        "  else:\n",
        "    x,y = x.to(device) , y.to(device)\n",
        "  return x.y\n"
      ],
      "metadata": {
        "id": "y89YKkvNqQOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from contextlib import nullcontext\n",
        "import os\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self,ndim,bias):\n",
        "    self.weight = nn.Parameter(torch.ones(ndim))\n",
        "    self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "  def forward(self,x):\n",
        "    return F.layer_norm(x,self.weight.shape , self.weight,self.bias , 1e-5)\n",
        "\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super.__init__()\n",
        "    assert config.n_embed % config.n_head == 0\n",
        "    self.c_attn = nn.Linear(config.n_embed,3*config.n_embed,bias= config.bias)\n",
        "    self.c_proj = nn.Linear(config.n_embed,config.n_embed,bias = config.bias)\n",
        "    self.nattn_dropout = nn.Dropout(config.dropout)\n",
        "    self.resid_dropout = nn.Dropout(config.dropout)\n",
        "    self.n_head = config.n_head\n",
        "    self.n_embed = config.n_embed\n",
        "    self.flash = hasattr(F,'scaled_dot_product_attention')\n",
        "    if not self.flash:\n",
        "      self.regiser_buffer(\"bias\",torch.tril(torch.ones(config.block_size,config.block_size))\n",
        "                                  .view(1,1,config.block_size,config.block_size))\n",
        "\n",
        "  def forward(self,x):\n",
        "    B , T, C = x.size()\n",
        "    q,k,v = self.c_attn(x).split(self.n_embed,dim=2)\n",
        "    k = k.view(B ,T ,self.n_head, C//self.n_head).transpose(1,2)\n",
        "    q = q.view(B ,T ,self.n_head, C//self.n_head).transpose(1,2)\n",
        "    v = v.view(B ,T ,self.n_head, C//self.n_head).transpose(1,2)\n",
        "\n",
        "\n",
        "    if self.flash:\n",
        "      y  = F.scaled_dot_product_attention(q,k,v,attn_mask= None,dropout_p = self.attn_dropout.p id self.training else 0.0 else is_causal = True)\n",
        "    else:\n",
        "      attn = (q@k.transpose(-2,-1))*(1.0/math.sqrt(k.size(-1)))\n",
        "      att = att.masked_fill(self.bias[:,:,:T,:T]==0 ,float('inf'))\n",
        "      att = F.softmax(att,dim=-1)\n",
        "      att = self.attn_droput(att)\n",
        "      y = att@v\n",
        "\n",
        "    y = y.transpose(1,2).contiguous().view(B,T,C)\n",
        "    y = self.resid_dropout(self.c_proj(y))\n",
        "    return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embed, 4 * config.n_embed, bias=config.bias)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.c_proj = nn.Linear(4 * config.n_embed, config.n_embed, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "    def forward(self, x):\n",
        "        return self.dropout(self.c_proj(self.gelu(self.c_fc(x))))\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = LayerNorm(config.n_embd, config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln2 = LayerNorm(config.n_embd, config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int\n",
        "    vocab_size: int\n",
        "    n_layer: int\n",
        "    n_head: int\n",
        "    n_embd: int\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True\n",
        "\n",
        "class GPT(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.transformer = nn.ModuleDict(dict(\n",
        "        wte = nn.Embedding(config.vocab_size,config.n_embed),\n",
        "        wpe = nn.Embedding(config.block_size,config.n_embed),\n",
        "        dro p = nn.Dropout(config.dropout),\n",
        "        h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "        ln_f = LayerNorm(config.n_embed,config.bias),\n",
        "    ))\n",
        "    self.lm_head = nn.Linear(config.n_embed,config.vocab_size,bias = False)\n",
        "    self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "    self.apply(self._init_weights):\n",
        "    for pn,p in self.named_parameters():\n",
        "      if pn.endswith(\"c_proj.weight\"):\n",
        "        nn.init.normal_(p,mean =0.0,std =0.02/math.sqrt(2*config.n_layer))\n",
        "\n",
        "    def _init_weights(self,module):\n",
        "      if isinstance(module,nn.Linear):\n",
        "        nn.init.normal_(module.weight,mean-0.0,std =0.02)\n",
        "        if module.bias is not None:\n",
        "          nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module,nn.Embedding):\n",
        "          nn.init.normal_(module.weight,mean = 0.0,std = 0.02)\n",
        "\n",
        "    def forward(self,idx,targets = None):\n",
        "      device = idx.device\n",
        "      b, t = idx.size()\n",
        "      assert t<= self.config.block_size\n",
        "      pos = torch.arange(0,t,dtype = torch.long,device = device)\n",
        "\n",
        "      tok_emb = self.transformerr.wte(idx)\n",
        "      pos_emb = self.transformer.wpe(pos)\n",
        "      x = self.transformer.drop(tok_emb + pos_emb)\n",
        "      for block in self.transformer.h:\n",
        "        x = block(x)\n",
        "      x = self.transformer.ln_h(x)\n",
        "\n",
        "      if targets is not None:\n",
        "        logits = self.lm_head(x)\n",
        "        loss = F.cross_entropy(logits.view(-1,logits.size(-1)),targets.view(-1),ignore_index = -1)\n",
        "        return logits,loss\n",
        "      else:\n",
        "        return self.lm_head(x)\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self,idx,max_new_tokens,temperature=1.0,top_k = None):\n",
        "      for _ in range(max_new_tokens):\n",
        "        idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:,-self.config.block_size:]\n",
        "        logits = self(idx_cond)\n",
        "        logits = logits[:,-1,:]\n",
        "        if top_k is not None:\n",
        "          v  , _ torch.topk(logits,min(top_k,logits,size(-1)))\n",
        "          logits[logits<v[:,[-1]]] = - float('inf')\n",
        "        probs = F.softmax(logits,dim=-1)\n",
        "        idx_next = torch.multinomial(probs,num_samples=1)\n",
        "        idx = torch.cat((idx,idx_next),dim=1)\n",
        "      return idx"
      ],
      "metadata": {
        "id": "ZQTLxeRVrVPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = GPTConfig(\n",
        "    vocab_size=50257,\n",
        "    block_size=128,\n",
        "    n_layer=6,\n",
        "    n_head=6,\n",
        "    n_embd=384,\n",
        "    dropout=0.1,\n",
        "    bias=True\n",
        ")\n",
        "\n",
        "model = GPT(config)"
      ],
      "metadata": {
        "id": "Es-a3Z77gvsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_loss(model):\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    for split in ['train','val']:\n",
        "      losses = torch.zeros(eval_iters)\n",
        "      for k in range(eval_iters):\n",
        "        X,y = get_batch(split)\n",
        "        with ctx:\n",
        "          logits ,loss = model(X,y)\n",
        "        losses[k] = loss.item()\n",
        "      out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out"
      ],
      "metadata": {
        "id": "AIKkI1lhc750"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from contextlib import nullcontext\n",
        "\n",
        "\n",
        "learning_rate = 1e-4\n",
        "max_iters = 20000\n",
        "warmup_steps = 1000\n",
        "min_lr = 5e-4\n",
        "eval_ites = 500\n",
        "batch_size = 32\n",
        "block_size = 128\n",
        "\n",
        "gradient_accumulation_steps = 32\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device_type = \"cuda\" if \"cuda\" in device else \"cpu\"\n",
        "\n",
        "dtype = \"bfloat16\" if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else \"float16\"\n",
        "ptdtype = {\"float32\": torch.float32, \"bfloat16\": torch.bfloat16 , \"float16\": torch.float16 }[dtype]\n",
        "\n",
        "\n",
        "ctx = nullcontext() if device_type == \"cpu\" else torch.amp.autocast(device_type = device_type,dtype = dtype)\n",
        "\n",
        "torch.set_default_device(device)\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "C3lT-unEdn70"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}