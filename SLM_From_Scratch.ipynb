{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhydWwRmL9TccURpU6px1a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PARTHIBAN-007/SLM-From-Scratch/blob/main/SLM_From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wIV-xQ7oLNe"
      },
      "outputs": [],
      "source": [
        "!pip install datasets tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"roneneldan/TinyStories\")"
      ],
      "metadata": {
        "id": "XSylDa1foX_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "en = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "def process(example):\n",
        "  ids = enc.encode_ordinary(example['text'])\n",
        "  out = {'ids':ids,'len':len(ids)}\n",
        "  return out\n",
        "\n",
        "\n",
        "if not os.path.exists('train.bin'):\n",
        "  tokenized = ds.map(\n",
        "      process,\n",
        "      remove_columns = ['text'],\n",
        "      desc = \"tokenizing the splits\",\n",
        "      num_proc = 8\n",
        "  )\n",
        "\n",
        "  for split,dset in tokenized.items():\n",
        "    arr_len = np.sum(dset['len'],dtype = np.uint64)\n",
        "    filename = f'{split}.bin'\n",
        "    dtype = np.uint16\n",
        "    arr = np.memmap(filename,dtype = dtype,mode = \"w+\",shape = (arr_len,))\n",
        "    total_batches = 1024\n",
        "\n",
        "    idx = 0\n",
        "    for batch_idx in tqd(range(total_batches),desc = f'writing {filename}'):\n",
        "      batch = dset.shard(num_shards = total_batches,index = batch_idx , contiguous = True).with_format('numpy')\n",
        "      arr_batch=  np.concatenate(batch['ids'])\n",
        "      arr[idx:idx+len(arr_batch)] = arr_batch\n",
        "      idx += len(arr_batch)\n",
        "    arr.flush()\n"
      ],
      "metadata": {
        "id": "KSyY8hEJol-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "  if split == \"train\":\n",
        "    data = np.memmap('train.bin',dtype = np.uint16,mode = 'r')\n",
        "  else:\n",
        "    data = np.memmap('validation.bin',dtype=np.uint16,mode = 'r')\n",
        "  ix = torch.randint(len(data)-block_size,(batch_size,))\n",
        "  x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "  y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "\n",
        "  if device_type == \"cuda\":\n",
        "    x,y = x.pin_memory().to(device,non_blocking = True), y.pin_memory().to(device,non_blocking = True)\n",
        "  else:\n",
        "    x,y = x.to(device) , y.to(device)\n",
        "  return x.y\n"
      ],
      "metadata": {
        "id": "y89YKkvNqQOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from contextlib import nullcontext\n",
        "import os\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self,ndim,bias):\n",
        "    self.weight = nn.Parameter(torch.ones(ndim))\n",
        "    self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "  def forward(self,x):\n",
        "    return F.layer_norm(x,self.weight.shape , self.weight,self.bias , 1e-5)\n",
        "\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super.__init__()\n",
        "    assert config.n_embed % config.n_head == 0\n",
        "    self.c_attn = nn.Linear(config.n_embed,3*config.n_embed,bias= config.bias)\n",
        "    self.c_proj = nn/Linear(config.n_embed,config.n_embed,bias = config.bias)\n",
        "    self.nattn_dropout = nn.Dropout(config.dropout)\n",
        "    self.resid_dropout = nn.Dropout(config.dropout)\n",
        "    self.n_head = config.n_head\n",
        "    self.n_embed = config.n_embed\n",
        "    self.flash = hasattr(F,'scaled_dot_product_attention')\n",
        "    if not self.flash:\n",
        "      self.regiser_buffer(\"bias\",torch.tril(torch.ones(config.block_size,config.block_size))\n",
        "                                  .view(1,1,config.block_size,config.block_size))\n",
        "\n",
        "  def forward(self,x):\n",
        "    B , T, C = x.size()\n",
        "    q,k,v = self.c_attn(x).split(self.n_embed,dim=2)\n",
        "    k = k.view(B ,T ,self.n_head, c//self.n_head).transpose(1,2)\n",
        "    q = q.view(B ,T ,self.n_head, c//self.n_head).transpose(1,2)\n",
        "    v = v.view(B ,T ,self.n_head, c//self.n_head).transpose(1,2)\n",
        "\n",
        "\n",
        "    if self.flash:\n",
        "      y  = F.scaled)dot_product_attention(q,k,v,attn_mask= None,dropout_p = self.attn_dropout.p id self.training else 0.0 else is_causal = True)\n",
        "    else:\n",
        "      attn = (q@k.transpose(-2,-1))*(1.0/,ath.sqrt(k.size(-1)))\n",
        "      att = att.masked_fill(self,bias[:,:,:T,:T]==0 ,float('inf'))\n",
        "      att = F.softmax(att,dim=-1)\n",
        "      att = self.attn_droput(att)\n",
        "      y = att@v\n",
        "\n",
        "    y = y.transpose(1,2)contiguous().view(B,T,C)\n",
        "    y = self.resid_dropout(self.c_proj(y))\n",
        "    return y"
      ],
      "metadata": {
        "id": "ZQTLxeRVrVPH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}